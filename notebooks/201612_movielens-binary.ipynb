{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gui/.virtualenvs/tfrecs/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "original test\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "train\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "test\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "test_positive_only\n",
      "[1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n",
      "There are 19 distinct item features, with values like ['genre:unknown', 'genre:Action', 'genre:Adventure'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens\n",
    "\n",
    "data = fetch_movielens('movielens', indicator_features=False, genre_features=True)\n",
    "\n",
    "print('original train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('original test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "\n",
    "# binarizing traing examples as in the original lightfm paper to use the logistic loss\n",
    "data['train'].data = np.array([-1, 1])[1 * (data['train'].data >= 4)]\n",
    "data['test'].data = np.array([-1, 1])[1 * (data['test'].data >= 4)]\n",
    "\n",
    "# should keep only positive test interactions\n",
    "data['test_positive_only'] = data['test'].copy()\n",
    "data['test_positive_only'].data = 1 *(data['test_positive_only'].data>=1)\n",
    "data['test_positive_only'].eliminate_zeros()\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "test_positives = data['test_positive_only']\n",
    "\n",
    "print('train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "print('test_positive_only')\n",
    "print(np.unique(data['test_positive_only'].data))\n",
    "print(data['test_positive_only'].__repr__())\n",
    "\n",
    "item_features = data['item_features']\n",
    "tag_labels = data['item_feature_labels']\n",
    "print('There are %s distinct item features, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 3)\n",
      "(1586126, 2)\n",
      "(943,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item  user\n",
       "0     0     0\n",
       "1     1     0\n",
       "2     2     0\n",
       "3     3     0\n",
       "4     4     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict({\n",
    "        'user': train.row,\n",
    "        'item': train.col,\n",
    "        'rating': train.data,\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({\n",
    "        'user': test.row,\n",
    "        'item': test.col,\n",
    "        'rating': test.data,\n",
    "    })\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "test_user_ids = test_df.user.unique()\n",
    "all_user_ids = train_df.user.unique()\n",
    "all_item_ids = np.unique(data['item_features'].tocoo().row)\n",
    "\n",
    "def to_all_user_items(user_ids, item_ids):\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {'user': np.repeat(user_ids, len(item_ids)),\n",
    "         'item': np.tile(item_ids, len(user_ids))})\n",
    "\n",
    "all_user_items = to_all_user_items(all_user_ids, all_item_ids)\n",
    "print(all_user_items.shape)\n",
    "print(test_user_ids.shape)\n",
    "all_user_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensforflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_predictions_to_hits(all_user_items, all_predicted_values, ground_truth_user_items):\n",
    "    predicted_ratings = all_user_items.assign(predicted_rating=lambda _: all_predicted_values)\n",
    "    predicted_ranks = predicted_ratings.groupby('user')['predicted_rating'].rank(ascending=False, method='max')\n",
    "    predicted_ratings['rank'] = predicted_ranks.values - 1\n",
    "\n",
    "    ground_truth_hits = pd.merge(\n",
    "        left=ground_truth_user_items,\n",
    "        right=predicted_ratings,\n",
    "        on=['user', 'item'], how='left')\n",
    "    return ground_truth_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_predicted_hits(predict_function, ground_truth_df, split_size=1000):\n",
    "    user_ids = ground_truth_df.user.unique()\n",
    "    item_ids = ground_truth_df.item.unique()\n",
    "    user_ids_splits = np.array_split(user_ids, len(user_ids) / split_size)\n",
    "    user_items_splits = (to_all_user_items(user_ids_split, item_ids) for user_ids_split in user_ids_splits)\n",
    "    hits_for_user_splits = [all_predictions_to_hits(\n",
    "            split_user_items, \n",
    "            all_predicted_values=predict_function(split_user_items),\n",
    "            ground_truth_user_items=ground_truth_df[ground_truth_df.user.isin(split_user_items.user.unique())])\n",
    "        for split_user_items in user_items_splits]\n",
    "    return pd.concat(hits_for_user_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(predicted_ranks_df):\n",
    "    return predicted_ranks_df\\\n",
    "        .assign(rec_rank=lambda df:1 / (df['rank'] + 1))\\\n",
    "        .groupby('user')['rec_rank'].max()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(positives_df, batch_size, positive_ratio=.33):\n",
    "    n_positives = int(batch_size * positive_ratio)\n",
    "    n_negatives = batch_size - n_positives\n",
    "    negatives = pd.DataFrame.from_dict({\n",
    "        'user': np.random.choice(all_user_ids, replace=True, size=n_negatives),\n",
    "        'item': np.random.choice(all_item_ids, replace=True, size=n_negatives),\n",
    "        'rating': np.repeat(0, n_negatives)\n",
    "        })\n",
    "    return pd.concat([positives_df.sample(n_positives), negatives], axis=0)\n",
    "\n",
    "# if train has both positives and negatives\n",
    "def sample_batch(positives_and_negatives_df, batch_size):\n",
    "    batch_df = positives_and_negatives_df.sample(batch_size)\n",
    "    return batch_df.assign(rating = lambda df: np.maximum(df.rating, 0))\n",
    "\n",
    "test_samples = sample_batch(train_df, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'user_ids:0' shape=(?,) dtype=int32>: array([933, 682, 408, 706, 436], dtype=int32),\n",
       " <tf.Tensor 'item_ids:0' shape=(?,) dtype=int32>: array([532, 326, 126, 448, 411], dtype=int32),\n",
       " <tf.Tensor 'ratings:0' shape=(?,) dtype=float32>: array([0, 1, 1, 0, 0])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.to_feed_dict(test_samples, with_ratings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS, N_ITEMS = train.shape\n",
    "\n",
    "\n",
    "class Placeholders:\n",
    "    \n",
    "    def __init__(self, batch_size=None):\n",
    "        self.user_ids = tf.placeholder(tf.int32, shape=[batch_size], name='user_ids')\n",
    "        self.item_sparse_features = tf.sparse_placeholder(tf.int32, name='item_features')\n",
    "        self.item_ids = tf.placeholder(tf.int32, shape=[batch_size], name='item_ids')\n",
    "        self.ratings = tf.placeholder(tf.float32, shape=[batch_size], name='ratings')\n",
    "\n",
    "    def to_feed_dict(self, user_items_df, with_ratings=False):\n",
    "        features_dict = {\n",
    "            self.user_ids: user_items_df.user.values,\n",
    "            self.item_ids: user_items_df.item.values\n",
    "        }\n",
    "        \n",
    "        if with_ratings:\n",
    "            features_dict[self.ratings] = user_items_df.rating.values\n",
    "\n",
    "        return features_dict\n",
    "\n",
    "\n",
    "class UserItem2BinaryModel:\n",
    "    def __init__(self, dimensionality=30):\n",
    "        self.dimensionality = dimensionality\n",
    "        \n",
    "        with tf.name_scope('B'):\n",
    "            self.user_biases =  tf.Variable(tf.random_normal(shape=[N_USERS, 1], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('user_biases', self.user_biases)\n",
    "\n",
    "        with tf.name_scope('B'):\n",
    "            self.item_biases =  tf.Variable(tf.random_normal(shape=[N_ITEMS, 1], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('item_biases', self.item_biases)\n",
    "\n",
    "        with tf.name_scope('X'):\n",
    "            self.user_factors = tf.Variable(tf.random_normal([N_USERS, self.dimensionality], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('user_factors', self.user_factors)\n",
    "            \n",
    "        with tf.name_scope('Y'):\n",
    "            self.item_factors = tf.Variable(tf.random_normal([N_ITEMS, self.dimensionality], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('item_factors', self.item_factors)\n",
    "\n",
    "    def user_bias(self, user_ids):\n",
    "        with tf.name_scope('B_user'):\n",
    "            return tf.squeeze(tf.nn.embedding_lookup(params=self.user_biases, ids=user_ids), name='B_user')\n",
    "\n",
    "    def item_bias(self, item_ids):\n",
    "        with tf.name_scope('C_item'):\n",
    "            return tf.squeeze(tf.nn.embedding_lookup(params=self.item_biases, ids=item_ids), name='C_item')\n",
    "\n",
    "    def user_item_product(self, user_ids, item_ids):\n",
    "        with tf.name_scope('X_user'):\n",
    "            batch_user_factors = tf.squeeze(tf.nn.embedding_lookup(self.user_factors, user_ids))\n",
    "        with tf.name_scope('Y_item'):\n",
    "            batch_item_factors = tf.squeeze(tf.nn.embedding_lookup(self.item_factors, item_ids))\n",
    "        with tf.name_scope('dot'):\n",
    "            factors_prediction = tf.reduce_mean(\n",
    "                tf.mul(batch_user_factors, batch_item_factors), reduction_indices=1)\n",
    "        return factors_prediction\n",
    "\n",
    "    def predictions(self, user_ids, item_ids):\n",
    "        with tf.name_scope('inference'):\n",
    "            return tf.add(\n",
    "                self.user_item_product(user_ids, item_ids), \n",
    "                tf.add(self.user_bias(user_ids), self.item_bias(item_ids), name='biases'),\n",
    "                name='logits')\n",
    "                                 \n",
    "\n",
    "def log_loss(predictions, targets):\n",
    "    \"\"\" targets as one-hot encodings\n",
    "    \"\"\"\n",
    "    with tf.name_scope('log_loss'):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, targets=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Step 0: batch/test log loss = 0.693/0.692, train/test MRR = 0.237/0.053\n",
      "Step 20: batch/test log loss = 0.661/0.661, train/test MRR = 0.480/0.128\n",
      "Step 40: batch/test log loss = 0.633/0.643, train/test MRR = 0.496/0.137\n",
      "Step 60: batch/test log loss = 0.612/0.627, train/test MRR = 0.560/0.157\n",
      "Step 80: batch/test log loss = 0.598/0.610, train/test MRR = 0.546/0.143\n",
      "Step 100: batch/test log loss = 0.588/0.601, train/test MRR = 0.550/0.143\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_ITER = 101\n",
    "BATCH_SIZE = 1024\n",
    "N_STEP_SUMMARY = 20\n",
    "LOG_DIR = '/tmp/tfrecs_logs'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = UserItem2BinaryModel(dimensionality=10)\n",
    "    inputs = Placeholders()\n",
    "    \n",
    "    logits = model.predictions(inputs.user_ids, inputs.item_ids)\n",
    "    \n",
    "    loss = log_loss(logits, inputs.ratings)\n",
    "    tf.summary.scalar('train_loss', loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    test_summary = tf.summary.scalar('test_loss', loss)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "                \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        batch_samples = sample_batch(train_df, BATCH_SIZE)\n",
    "\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict=inputs.to_feed_dict(batch_samples, with_ratings=True))\n",
    "        \n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % N_STEP_SUMMARY == 0:\n",
    "\n",
    "            test_samples = sample_batch(test_df, BATCH_SIZE)\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict=inputs.to_feed_dict(test_samples, with_ratings=True))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "            # predicting on all users and items\n",
    "            all_prediction_values = logits.eval(feed_dict=inputs.to_feed_dict(all_user_items))\n",
    "            \n",
    "            print('Step %d: batch/test log loss = %.3f/%.3f, train/test MRR = %.3f/%.3f' % (\n",
    "                    step, loss_value, test_loss_value, \n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, train_df.query(\"rating > 0\"))).mean(),\n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, test_df.query(\"rating > 0\"))).mean()\n",
    "                ))\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR + '/{:%Y%m%d%H%M%S}'.format(dt.datetime.now()), sess.graph)\n",
    "\n",
    "        print('Starting training')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train_df, test_df, summary_writer)\n",
    "\n",
    "        train_hits = all_predicted_hits(\n",
    "            lambda user_items: logits.eval(inputs.to_feed_dict(user_items)),\n",
    "            train_df, split_size=100)\n",
    "\n",
    "        test_hits = all_predicted_hits(\n",
    "            lambda user_items: logits.eval(inputs.to_feed_dict(user_items)),\n",
    "            test_df.query(\"rating > 0\"), split_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 5)\n",
      "(5469, 5)\n",
      "0.502520948841\n",
      "0.112540172658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>user</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146599</td>\n",
       "      <td>412.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.150742</td>\n",
       "      <td>669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379277</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048568</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.455228</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item  rating  user  predicted_rating   rank\n",
       "0    19       1     0          0.146599  412.0\n",
       "1    32       1     0         -0.150742  669.0\n",
       "2    60       1     0          0.379277  254.0\n",
       "3   159       1     0          0.048568  512.0\n",
       "4   170       1     0          0.455228  199.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_hits.shape)\n",
    "print(test_hits.shape)\n",
    "        \n",
    "print(mean_reciprocal_rank(train_hits).mean())\n",
    "print(mean_reciprocal_rank(test_hits).mean())\n",
    "test_hits.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943,) 0.162602 0.847304\n",
      "(934,) 0.164169242148\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def to_sparse_ranks(df, dtype=np.float32):\n",
    "    return sparse.csr_matrix((df['rank'], (df.user, df.item)), dtype=dtype)\n",
    "\n",
    "from lightfm._lightfm_fast import CSRMatrix, calculate_auc_from_rank\n",
    "\n",
    "def hits_to_auc(test_hits):\n",
    "    \"\"\"\n",
    "    Simplification of https://github.com/lyst/lightfm/blob/master/lightfm/evaluation.py#L136\n",
    "    \"\"\"\n",
    "    ranks = to_sparse_ranks(test_hits)\n",
    "    auc = np.zeros(ranks.shape[0], dtype=np.float32)\n",
    "    num_train_positives = np.zeros(ranks.shape[0], dtype=np.int32)\n",
    "    calculate_auc_from_rank(CSRMatrix(sparse.csr_matrix(ranks, dtype=np.float32)), num_train_positives, ranks.data, auc, num_threads=1)\n",
    "\n",
    "    return auc\n",
    "\n",
    "def hits_to_mrr(test_hits):\n",
    "    \"\"\" From https://github.com/lyst/lightfm/blob/master/lightfm/evaluation.py#L206\n",
    "    \"\"\"\n",
    "    ranks = to_sparse_ranks(test_hits)\n",
    "\n",
    "    ranks.data = 1.0 / (ranks.data + 1.0)\n",
    "    return np.squeeze(np.array(ranks.max(axis=1).todense()))\n",
    "\n",
    "\n",
    "print(\n",
    "    hits_to_mrr(test_hits).shape,\n",
    "    hits_to_mrr(test_hits).mean(),\n",
    "    hits_to_auc(test_hits).mean())\n",
    "\n",
    "# TODO investigate why test_hits doesn't cover all test users\n",
    "print(mean_reciprocal_rank(test_hits).shape, mean_reciprocal_rank(test_hits).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Step 0: batch/test log loss = 7.428/7.427, train/test MRR = 0.095/0.026\n",
      "Step 20: batch/test log loss = 7.408/7.410, train/test MRR = 0.362/0.085\n",
      "Step 40: batch/test log loss = 7.378/7.385, train/test MRR = 0.392/0.098\n",
      "Step 60: batch/test log loss = 7.301/7.339, train/test MRR = 0.409/0.107\n",
      "Step 80: batch/test log loss = 7.184/7.262, train/test MRR = 0.469/0.123\n",
      "Step 100: batch/test log loss = 7.022/7.152, train/test MRR = 0.535/0.153\n",
      "Step 120: batch/test log loss = 6.851/7.060, train/test MRR = 0.488/0.132\n",
      "Step 140: batch/test log loss = 6.752/6.963, train/test MRR = 0.507/0.134\n",
      "Step 160: batch/test log loss = 6.634/6.887, train/test MRR = 0.574/0.157\n",
      "Step 180: batch/test log loss = 6.555/6.850, train/test MRR = 0.578/0.164\n",
      "Step 200: batch/test log loss = 6.547/6.831, train/test MRR = 0.569/0.164\n"
     ]
    }
   ],
   "source": [
    "class User2MulticClassItemsModel:\n",
    "    def __init__(self, dimensionality=10):\n",
    "        self.dimensionality = dimensionality\n",
    "            \n",
    "        with tf.name_scope('item_biases'):\n",
    "            self.item_biases =  tf.Variable(tf.random_normal(shape=[N_ITEMS], stddev=0.01, mean=0), name='item_features_biases')\n",
    "            tf.summary.histogram('item_biases', self.item_biases)\n",
    "\n",
    "        with tf.name_scope('user_factors'):\n",
    "            self.user_factors = tf.Variable(tf.random_normal([N_USERS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('user_factors', self.user_factors)\n",
    "            \n",
    "        with tf.name_scope('item_factors'):\n",
    "            self.item_factors = tf.Variable(tf.random_normal([N_ITEMS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('item_factors', self.item_factors)\n",
    "\n",
    "    def output_items_scores(self, user_ids):\n",
    "        with tf.name_scope('user_item_product'):\n",
    "            return self.item_biases + tf.matmul(\n",
    "                tf.nn.embedding_lookup(self.user_factors, user_ids),\n",
    "                tf.transpose(self.item_factors), name='user_to_all_items_logits')\n",
    "\n",
    "def multiclass_loss(logits, target_item_ids):\n",
    "    with tf.name_scope('loss'):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits,\n",
    "                labels=target_item_ids))\n",
    "    \n",
    "import os\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "N_ITER = 201\n",
    "BATCH_SIZE = 1024\n",
    "N_STEP_SUMMARY = 20\n",
    "LOG_DIR = '/tmp/tf_logs'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputs = Placeholders()\n",
    "        \n",
    "    model = User2MulticClassItemsModel(30)\n",
    "    logits = model.output_items_scores(inputs.user_ids)\n",
    "    loss = multiclass_loss(logits, inputs.item_ids)\n",
    "    \n",
    "    tf.summary.scalar('train_loss', loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    test_summary = tf.summary.scalar('test_loss', loss)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "                \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        batch_samples = train_df.query(\"rating == 1\").sample(BATCH_SIZE)\n",
    "\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict=inputs.to_feed_dict(batch_samples, with_ratings=False))\n",
    "        \n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % N_STEP_SUMMARY == 0:\n",
    "\n",
    "            test_samples = sample_batch(test_df, BATCH_SIZE)\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict=inputs.to_feed_dict(test_samples, with_ratings=True))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "            # predicting on all users and items\n",
    "            all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "            print('Step %d: batch/test log loss = %.3f/%.3f, train/test MRR = %.3f/%.3f' % (\n",
    "                    step, loss_value, test_loss_value, \n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, train_df.query(\"rating > 0\"))).mean(),\n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, test_df.query(\"rating > 0\"))).mean()\n",
    "                ))\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR + '/{:%Y%m%d%H%M%S}'.format(dt.datetime.now()), sess.graph)\n",
    "\n",
    "        print('Starting training')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train_df, test_df, summary_writer)\n",
    "\n",
    "        all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "        train_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            train_df.query(\"rating > 0\"))\n",
    "\n",
    "        test_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            test_df.query(\"rating > 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943,) 0.162602 0.847304\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    hits_to_mrr(test_hits).shape,\n",
    "    hits_to_mrr(test_hits).mean(),\n",
    "    hits_to_auc(test_hits).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>median</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reciprocal_rank</th>\n",
       "      <td>0.500372</td>\n",
       "      <td>0.384439</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_score</th>\n",
       "      <td>0.844125</td>\n",
       "      <td>0.115616</td>\n",
       "      <td>0.873672</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_at_k</th>\n",
       "      <td>0.178155</td>\n",
       "      <td>0.131675</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean       std    median  size\n",
       "reciprocal_rank  0.500372  0.384439  0.333333   943\n",
       "auc_score        0.844125  0.115616  0.873672   943\n",
       "recall_at_k      0.178155  0.131675  0.200000   943"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightfm.evaluation import reciprocal_rank, auc_score, recall_at_k\n",
    "\n",
    "score_functions = [reciprocal_rank, auc_score, recall_at_k]\n",
    "stat_functions = [np.mean, np.std, np.median, np.size]\n",
    "\n",
    "def scores_stats(scores, stat_functions):\n",
    "    return [f(scores) for f in stat_functions]\n",
    "\n",
    "pd.DataFrame(\n",
    "    index=[s.__name__ for s in score_functions],\n",
    "    data=[scores_stats(\n",
    "        score(cf_model, \n",
    "              test_interactions=data['test'], train_interactions=data['train']),\n",
    "        stat_functions)\n",
    "        for score in score_functions],\n",
    "    columns=[s.__name__ for s in stat_functions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gui/.virtualenvs/tfrecs/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-720d07731f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightFM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m print('Collaborative filtering train/test MRR: %.3f / %.3f'\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import reciprocal_rank, auc_score, recall_at_k, precision_at_k\n",
    "\n",
    "cf_model = LightFM(loss='logistic', item_alpha=0.0001, no_components=10, learning_rate=0.001)\n",
    "cf_model.fit(train, epochs=20)\n",
    "\n",
    "print('Collaborative filtering train/test MRR: %.3f / %.3f'\n",
    "      % (reciprocal_rank(cf_model, data['train']).mean(),\n",
    "         reciprocal_rank(cf_model, data['test']).mean()))\n",
    "\n",
    "print('Collaborative filtering train/test AUC: %.3f / %.3f'\n",
    "      % (auc_score(cf_model, data['train']).mean(),\n",
    "         auc_score(cf_model, data['test'], train_interactions=None).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering train/test MRR: 0.807 / 0.254\n",
      "Collaborative filtering train/test AUC: 0.853 / 0.826\n"
     ]
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import reciprocal_rank, auc_score, recall_at_k\n",
    "\n",
    "cf_model = LightFM(loss='bpr', item_alpha=0.0001, no_components=20)\n",
    "cf_model.fit(train, epochs=10)\n",
    "\n",
    "print('Collaborative filtering train/test MRR: %.3f / %.3f'\n",
    "      % (reciprocal_rank(cf_model, data['train']).mean(),\n",
    "         reciprocal_rank(cf_model, data['test']).mean()))\n",
    "\n",
    "print('Collaborative filtering train/test AUC: %.3f / %.3f'\n",
    "      % (auc_score(cf_model, data['train']).mean(),\n",
    "         auc_score(cf_model, data['test'], train_interactions=None).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering train/test MRR: 0.825 / 0.272\n",
      "Collaborative filtering train/test AUC: 0.922 / 0.889\n"
     ]
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import reciprocal_rank, auc_score\n",
    "\n",
    "cf_model = LightFM(loss='warp', item_alpha=0.0001, no_components=20)\n",
    "cf_model.fit(train, epochs=10)\n",
    "\n",
    "print('Collaborative filtering train/test MRR: %.3f / %.3f'\n",
    "      % (reciprocal_rank(cf_model, data['train']).mean(),\n",
    "         reciprocal_rank(cf_model, data['test']).mean()))\n",
    "\n",
    "print('Collaborative filtering train/test AUC: %.3f / %.3f'\n",
    "      % (auc_score(cf_model, data['train']).mean(),\n",
    "         auc_score(cf_model, data['test'], train_interactions=None).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
