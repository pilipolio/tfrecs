{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gui/.virtualenvs/tfrecs/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "original test\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "train\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "test\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "test_positive_only\n",
      "[1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n",
      "There are 19 distinct item features, with values like ['genre:unknown', 'genre:Action', 'genre:Adventure'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens\n",
    "\n",
    "data = fetch_movielens('movielens', indicator_features=False, genre_features=True)\n",
    "\n",
    "print('original train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('original test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "\n",
    "# binarizing traing examples as in the original lightfm paper to use the logistic loss\n",
    "data['train'].data = np.array([-1, 1])[1 * (data['train'].data >= 4)]\n",
    "data['test'].data = np.array([-1, 1])[1 * (data['test'].data >= 4)]\n",
    "\n",
    "# should keep only positive test interactions\n",
    "data['test_positive_only'] = data['test'].copy()\n",
    "data['test_positive_only'].data = 1 *(data['test_positive_only'].data>=1)\n",
    "data['test_positive_only'].eliminate_zeros()\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "test_positives = data['test_positive_only']\n",
    "\n",
    "print('train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "print('test_positive_only')\n",
    "print(np.unique(data['test_positive_only'].data))\n",
    "print(data['test_positive_only'].__repr__())\n",
    "\n",
    "item_features = data['item_features']\n",
    "tag_labels = data['item_feature_labels']\n",
    "print('There are %s distinct item features, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 3)\n",
      "(1586126, 2)\n",
      "(943,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item  user\n",
       "0     0     0\n",
       "1     1     0\n",
       "2     2     0\n",
       "3     3     0\n",
       "4     4     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict({\n",
    "        'user': train.row,\n",
    "        'item': train.col,\n",
    "        'rating': train.data,\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({\n",
    "        'user': test.row,\n",
    "        'item': test.col,\n",
    "        'rating': test.data,\n",
    "    })\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "test_user_ids = test_df.user.unique()\n",
    "all_user_ids = train_df.user.unique()\n",
    "all_item_ids = np.unique(data['item_features'].tocoo().row)\n",
    "\n",
    "def to_all_user_items(user_ids, item_ids):\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {'user': np.repeat(user_ids, len(item_ids)),\n",
    "         'item': np.tile(item_ids, len(user_ids))})\n",
    "\n",
    "all_user_items = to_all_user_items(all_user_ids, all_item_ids)\n",
    "print(all_user_items.shape)\n",
    "print(test_user_ids.shape)\n",
    "all_user_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensforflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_predictions_to_hits(all_user_items, all_predicted_values, ground_truth_user_items):\n",
    "    predicted_ratings = all_user_items.assign(predicted_rating=lambda _: all_predicted_values)\n",
    "    predicted_ranks = predicted_ratings.groupby('user')['predicted_rating'].rank(ascending=False, method='max')\n",
    "    predicted_ratings['rank'] = predicted_ranks.values - 1\n",
    "\n",
    "    ground_truth_hits = pd.merge(\n",
    "        left=ground_truth_user_items,\n",
    "        right=predicted_ratings,\n",
    "        on=['user', 'item'], how='left')\n",
    "    return ground_truth_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_predicted_hits(predict_function, ground_truth_df, split_size=1000):\n",
    "    user_ids = ground_truth_df.user.unique()\n",
    "    item_ids = ground_truth_df.item.unique()\n",
    "    user_ids_splits = np.array_split(user_ids, len(user_ids) / split_size)\n",
    "    user_items_splits = (to_all_user_items(user_ids_split, item_ids) for user_ids_split in user_ids_splits)\n",
    "    hits_for_user_splits = [all_predictions_to_hits(\n",
    "            split_user_items, \n",
    "            all_predicted_values=predict_function(split_user_items),\n",
    "            ground_truth_user_items=ground_truth_df[ground_truth_df.user.isin(split_user_items.user.unique())])\n",
    "        for split_user_items in user_items_splits]\n",
    "    return pd.concat(hits_for_user_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(predicted_ranks_df):\n",
    "    return predicted_ranks_df\\\n",
    "        .assign(rec_rank=lambda df:1 / (df['rank'] + 1))\\\n",
    "        .groupby('user')['rec_rank'].max()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_batch(positives_df, batch_size, positive_ratio=.33):\n",
    "    n_positives = int(batch_size * positive_ratio)\n",
    "    n_negatives = batch_size - n_positives\n",
    "    negatives = pd.DataFrame.from_dict({\n",
    "        'user': np.random.choice(all_user_ids, replace=True, size=n_negatives),\n",
    "        'item': np.random.choice(all_item_ids, replace=True, size=n_negatives),\n",
    "        'rating': np.repeat(0, n_negatives)\n",
    "        })\n",
    "    return pd.concat([positives_df.sample(n_positives), negatives], axis=0)\n",
    "\n",
    "# if train has both positives and negatives\n",
    "def sample_batch(positives_and_negatives_df, batch_size):\n",
    "    batch_df = positives_and_negatives_df.sample(batch_size)\n",
    "    return batch_df.assign(rating = lambda df: np.maximum(df.rating, 0))\n",
    "\n",
    "test_samples = sample_batch(train_df, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Step 0: batch/test log loss = 7.427/7.419, train/test MRR = 0.319/0.067\n",
      "Step 20: batch/test log loss = 7.115/7.174, train/test MRR = 0.439/0.129\n",
      "Step 40: batch/test log loss = 6.534/6.820, train/test MRR = 0.553/0.158\n",
      "Step 60: batch/test log loss = 6.529/6.740, train/test MRR = 0.585/0.166\n",
      "Step 80: batch/test log loss = 6.433/6.707, train/test MRR = 0.601/0.171\n",
      "Step 100: batch/test log loss = 6.474/6.721, train/test MRR = 0.605/0.177\n",
      "Step 120: batch/test log loss = 6.442/6.661, train/test MRR = 0.605/0.175\n",
      "Step 140: batch/test log loss = 6.383/6.598, train/test MRR = 0.625/0.191\n",
      "Step 160: batch/test log loss = 6.334/6.618, train/test MRR = 0.647/0.196\n",
      "Step 180: batch/test log loss = 6.299/6.586, train/test MRR = 0.679/0.211\n",
      "Step 200: batch/test log loss = 6.254/6.496, train/test MRR = 0.717/0.212\n"
     ]
    }
   ],
   "source": [
    "N_USERS, N_ITEMS = train.shape\n",
    "\n",
    "\n",
    "class Placeholders:\n",
    "    \n",
    "    def __init__(self, batch_size=None):\n",
    "        self.user_ids = tf.placeholder(tf.int32, shape=[batch_size], name='user_ids')\n",
    "        self.item_ids = tf.placeholder(tf.int32, shape=[batch_size], name='item_ids')\n",
    "        self.ratings = tf.placeholder(tf.float32, shape=[batch_size], name='ratings')\n",
    "\n",
    "    def to_feed_dict(self, user_items_df, with_ratings=False):\n",
    "        features_dict = {\n",
    "            self.user_ids: user_items_df.user.values,\n",
    "            self.item_ids: user_items_df.item.values\n",
    "        }\n",
    "        \n",
    "        if with_ratings:\n",
    "            features_dict[self.ratings] = user_items_df.rating.values\n",
    "\n",
    "        return features_dict\n",
    "\n",
    "    \n",
    "class User2MultiClassItemsModel:\n",
    "    def __init__(self, dimensionality):\n",
    "        self.dimensionality = dimensionality\n",
    "            \n",
    "        with tf.name_scope('B'):\n",
    "            self.item_biases =  tf.Variable(tf.random_normal(shape=[N_ITEMS], stddev=0.01, mean=0), name='item_biases')\n",
    "            tf.summary.histogram('item_biases', self.item_biases)\n",
    "\n",
    "        with tf.name_scope('Q'):\n",
    "            self.user_factors = tf.Variable(tf.random_normal([N_USERS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('user_factors', self.user_factors)\n",
    "            \n",
    "        with tf.name_scope('P'):\n",
    "            self.item_factors = tf.Variable(tf.random_normal([N_ITEMS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('item_factors', self.item_factors)\n",
    "       \n",
    "    def predictions(self, user_ids):\n",
    "        with tf.name_scope('inference'):\n",
    "            with tf.name_scope('Q_user'):\n",
    "                batch_user_factors = tf.nn.embedding_lookup(self.user_factors, user_ids)\n",
    "            with tf.name_scope('all_items_logits'):\n",
    "                return tf.matmul(batch_user_factors, tf.transpose(self.item_factors)) + self.item_biases\n",
    "\n",
    "def cross_entropy_loss(logits, target_item_ids):\n",
    "    with tf.name_scope('cross_entropy_loss'):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits,\n",
    "                labels=target_item_ids))\n",
    "    \n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_ITER = 201\n",
    "BATCH_SIZE = 1024\n",
    "N_STEP_SUMMARY = 20\n",
    "LOG_DIR = '/tmp/tfrecs_logs'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputs = Placeholders()\n",
    "        \n",
    "    model = User2MultiClassItemsModel(10)\n",
    "    logits = model.predictions(inputs.user_ids)\n",
    "    loss = cross_entropy_loss(logits, inputs.item_ids)\n",
    "    \n",
    "    tf.summary.scalar('train_loss', loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    test_summary = tf.summary.scalar('test_loss', loss)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "                \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        batch_samples = train_df.query(\"rating == 1\").sample(BATCH_SIZE)\n",
    "\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict=inputs.to_feed_dict(batch_samples))\n",
    "        \n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % N_STEP_SUMMARY == 0:\n",
    "\n",
    "            test_samples = sample_batch(test_df, BATCH_SIZE)\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict=inputs.to_feed_dict(test_samples))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "            # predicting on all users and items\n",
    "            all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "            print('Step %d: batch/test log loss = %.3f/%.3f, train/test MRR = %.3f/%.3f' % (\n",
    "                    step, loss_value, test_loss_value, \n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, train_df.query(\"rating > 0\"))).mean(),\n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, test_df.query(\"rating > 0\"))).mean()\n",
    "                ))\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR + '/{:%Y%m%d%H%M%S}'.format(dt.datetime.now()), sess.graph)\n",
    "\n",
    "        print('Starting training')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train_df, test_df, summary_writer)\n",
    "\n",
    "        all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "        train_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            train_df.query(\"rating > 0\"))\n",
    "\n",
    "        test_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            test_df.query(\"rating > 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49906, 5)\n",
      "(5469, 5)\n",
      "0.692662021675\n",
      "0.217270357883\n"
     ]
    }
   ],
   "source": [
    "print(train_hits.shape)\n",
    "print(test_hits.shape)\n",
    "print(mean_reciprocal_rank(train_hits).mean())\n",
    "print(mean_reciprocal_rank(test_hits).mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
