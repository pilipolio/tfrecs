{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allaingu/miniconda3/envs/gui/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "original test\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "train\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "test\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "test_positive_only\n",
      "[1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n",
      "There are 19 distinct item features, with values like ['genre:unknown', 'genre:Action', 'genre:Adventure'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens\n",
    "\n",
    "data = fetch_movielens('movielens', indicator_features=False, genre_features=True)\n",
    "\n",
    "print('original train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('original test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "\n",
    "# binarizing traing examples as in the original lightfm paper to use the logistic loss\n",
    "data['train'].data = np.array([-1, 1])[1 * (data['train'].data >= 4)]\n",
    "data['test'].data = np.array([-1, 1])[1 * (data['test'].data >= 4)]\n",
    "\n",
    "# should keep only positive test interactions\n",
    "data['test_positive_only'] = data['test'].copy()\n",
    "data['test_positive_only'].data = 1 *(data['test_positive_only'].data>=1)\n",
    "data['test_positive_only'].eliminate_zeros()\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "test_positives = data['test_positive_only']\n",
    "\n",
    "print('train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "print('test_positive_only')\n",
    "print(np.unique(data['test_positive_only'].data))\n",
    "print(data['test_positive_only'].__repr__())\n",
    "\n",
    "item_features = data['item_features']\n",
    "tag_labels = data['item_feature_labels']\n",
    "print('There are %s distinct item features, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 3)\n",
      "(1586126, 2)\n",
      "(943,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item\n",
       "0     0     0\n",
       "1     0     1\n",
       "2     0     2\n",
       "3     0     3\n",
       "4     0     4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict({\n",
    "        'user': train.row,\n",
    "        'item': train.col,\n",
    "        'rating': train.data,\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({\n",
    "        'user': test.row,\n",
    "        'item': test.col,\n",
    "        'rating': test.data,\n",
    "    })\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "test_user_ids = test_df.user.unique()\n",
    "all_user_ids = train_df.user.unique()\n",
    "all_item_ids = np.unique(data['item_features'].tocoo().row)\n",
    "\n",
    "def to_all_user_items(user_ids, item_ids):\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {'user': np.repeat(user_ids, len(item_ids)),\n",
    "         'item': np.tile(item_ids, len(user_ids))})\n",
    "\n",
    "all_user_items = to_all_user_items(all_user_ids, all_item_ids)\n",
    "print(all_user_items.shape)\n",
    "print(test_user_ids.shape)\n",
    "all_user_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensforflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allaingu/miniconda3/envs/gui/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_predictions_to_hits(all_user_items, all_predicted_values, ground_truth_user_items):\n",
    "    predicted_ratings = all_user_items.assign(predicted_rating=lambda _: all_predicted_values)\n",
    "    predicted_ranks = predicted_ratings.groupby('user')['predicted_rating'].rank(ascending=False, method='max')\n",
    "    predicted_ratings['rank'] = predicted_ranks.values - 1\n",
    "\n",
    "    ground_truth_hits = pd.merge(\n",
    "        left=ground_truth_user_items,\n",
    "        right=predicted_ratings,\n",
    "        on=['user', 'item'], how='left')\n",
    "    return ground_truth_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_predicted_hits(predict_function, ground_truth_df, split_size=1000):\n",
    "    user_ids = ground_truth_df.user.unique()\n",
    "    item_ids = ground_truth_df.item.unique()\n",
    "    user_ids_splits = np.array_split(user_ids, len(user_ids) / split_size)\n",
    "    user_items_splits = (to_all_user_items(user_ids_split, item_ids) for user_ids_split in user_ids_splits)\n",
    "    hits_for_user_splits = [all_predictions_to_hits(\n",
    "            split_user_items, \n",
    "            all_predicted_values=predict_function(split_user_items),\n",
    "            ground_truth_user_items=ground_truth_df[ground_truth_df.user.isin(split_user_items.user.unique())])\n",
    "        for split_user_items in user_items_splits]\n",
    "    return pd.concat(hits_for_user_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(predicted_ranks_df):\n",
    "    return predicted_ranks_df\\\n",
    "        .assign(rec_rank=lambda df:1 / (df['rank'] + 1))\\\n",
    "        .groupby('user')['rec_rank'].max()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(positives_df, batch_size, positive_ratio=.33):\n",
    "    n_positives = int(batch_size * positive_ratio)\n",
    "    n_negatives = batch_size - n_positives\n",
    "    negatives = pd.DataFrame.from_dict({\n",
    "        'user': np.random.choice(all_user_ids, replace=True, size=n_negatives),\n",
    "        'item': np.random.choice(all_item_ids, replace=True, size=n_negatives),\n",
    "        'rating': np.repeat(0, n_negatives)\n",
    "        })\n",
    "    return pd.concat([positives_df.sample(n_positives), negatives], axis=0)\n",
    "\n",
    "# if train has both positives and negatives\n",
    "def sample_batch(positives_and_negatives_df, batch_size):\n",
    "    batch_df = positives_and_negatives_df.sample(batch_size)\n",
    "    return batch_df.assign(rating = lambda df: np.maximum(df.rating, 0))\n",
    "\n",
    "test_samples = sample_batch(train_df, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Starting training\n",
      "test\n",
      "Step 0: batch/test log loss = 7.428/7.420, train/test MRR = 0.139/0.048\n",
      "test\n",
      "Step 20: batch/test log loss = 7.154/7.196, train/test MRR = 0.476/0.127\n",
      "test\n",
      "Step 40: batch/test log loss = 6.630/6.834, train/test MRR = 0.508/0.146\n",
      "test\n",
      "Step 60: batch/test log loss = 6.443/6.720, train/test MRR = 0.560/0.164\n",
      "test\n",
      "Step 80: batch/test log loss = 6.418/6.694, train/test MRR = 0.579/0.169\n",
      "test\n",
      "Step 100: batch/test log loss = 6.450/6.691, train/test MRR = 0.589/0.179\n",
      "test\n",
      "Step 120: batch/test log loss = 6.464/6.690, train/test MRR = 0.592/0.182\n",
      "test\n",
      "Step 140: batch/test log loss = 6.403/6.620, train/test MRR = 0.614/0.196\n",
      "test\n",
      "Step 160: batch/test log loss = 6.344/6.627, train/test MRR = 0.638/0.201\n",
      "test\n",
      "Step 180: batch/test log loss = 6.306/6.511, train/test MRR = 0.655/0.209\n",
      "test\n",
      "Step 200: batch/test log loss = 6.235/6.454, train/test MRR = 0.678/0.214\n",
      "test\n",
      "Step 220: batch/test log loss = 6.238/6.474, train/test MRR = 0.710/0.214\n",
      "test\n",
      "Step 240: batch/test log loss = 6.128/6.373, train/test MRR = 0.719/0.211\n",
      "test\n",
      "Step 260: batch/test log loss = 6.058/6.347, train/test MRR = 0.749/0.215\n",
      "test\n",
      "Step 280: batch/test log loss = 6.058/6.271, train/test MRR = 0.759/0.214\n",
      "test\n",
      "Step 300: batch/test log loss = 6.026/6.213, train/test MRR = 0.771/0.214\n",
      "test\n",
      "Step 320: batch/test log loss = 6.027/6.285, train/test MRR = 0.771/0.207\n",
      "test\n",
      "Step 340: batch/test log loss = 5.954/6.229, train/test MRR = 0.774/0.205\n",
      "test\n",
      "Step 360: batch/test log loss = 6.014/6.231, train/test MRR = 0.784/0.208\n",
      "test\n",
      "Step 380: batch/test log loss = 5.907/6.214, train/test MRR = 0.798/0.197\n",
      "test\n",
      "Step 400: batch/test log loss = 5.867/6.266, train/test MRR = 0.800/0.197\n"
     ]
    }
   ],
   "source": [
    "N_USERS, N_ITEMS = train.shape\n",
    "\n",
    "\n",
    "class Placeholders:\n",
    "    \n",
    "    def __init__(self, batch_size=None):\n",
    "        self.user_ids = tf.placeholder(tf.int32, shape=[batch_size], name='user_ids')\n",
    "        self.item_ids = tf.placeholder(tf.int32, shape=[batch_size], name='item_ids')\n",
    "        self.ratings = tf.placeholder(tf.float32, shape=[batch_size], name='ratings')\n",
    "\n",
    "    def to_feed_dict(self, user_items_df, with_ratings=False):\n",
    "        features_dict = {\n",
    "            self.user_ids: user_items_df.user.values,\n",
    "            self.item_ids: user_items_df.item.values\n",
    "        }\n",
    "        \n",
    "        if with_ratings:\n",
    "            features_dict[self.ratings] = user_items_df.rating.values\n",
    "\n",
    "        return features_dict\n",
    "\n",
    "    \n",
    "class User2MultiClassItemsModel:\n",
    "    def __init__(self, dimensionality):\n",
    "        self.dimensionality = dimensionality\n",
    "            \n",
    "        with tf.name_scope('B'):\n",
    "            self.item_biases =  tf.Variable(tf.random_normal(shape=[N_ITEMS], stddev=0.01, mean=0), name='item_biases')\n",
    "            tf.summary.histogram('item_biases', self.item_biases)\n",
    "\n",
    "        with tf.name_scope('Q'):\n",
    "            self.user_factors = tf.Variable(tf.random_normal([N_USERS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('user_factors', self.user_factors)\n",
    "            \n",
    "        with tf.name_scope('P'):\n",
    "            self.item_factors = tf.Variable(tf.random_normal([N_ITEMS, self.dimensionality], stddev=0.01, mean=0), name='users')\n",
    "            tf.summary.histogram('item_factors', self.item_factors)\n",
    "       \n",
    "    def predictions(self, user_ids):\n",
    "        with tf.name_scope('inference'):\n",
    "            with tf.name_scope('Q_user'):\n",
    "                batch_user_factors = tf.nn.embedding_lookup(self.user_factors, user_ids)\n",
    "            with tf.name_scope('all_items_logits'):\n",
    "                return tf.matmul(batch_user_factors, tf.transpose(self.item_factors)) + self.item_biases\n",
    "\n",
    "            \n",
    "def cross_entropy_loss(logits, target_item_ids):\n",
    "    with tf.name_scope('cross_entropy_loss'):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logits,\n",
    "                labels=target_item_ids))\n",
    "    \n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_ITER = 401\n",
    "BATCH_SIZE = 1024\n",
    "N_STEP_SUMMARY = 20\n",
    "LOG_DIR = '/tmp/tfrecs_logs'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputs = Placeholders()\n",
    "        \n",
    "    model = User2MultiClassItemsModel(10)\n",
    "    logits = model.predictions(inputs.user_ids)\n",
    "    loss = cross_entropy_loss(logits, inputs.item_ids)\n",
    "\n",
    "    tf.summary.scalar('train_loss', loss)\n",
    "\n",
    "    summary = tf.summary.merge_all()\n",
    "    test_summary = tf.summary.scalar('test_loss', loss)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        batch_samples = train_df.query(\"rating == 1\").sample(BATCH_SIZE)\n",
    "\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict=inputs.to_feed_dict(batch_samples))\n",
    "        \n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % N_STEP_SUMMARY == 0:\n",
    "            test_samples = sample_batch(test_df, BATCH_SIZE)\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict=inputs.to_feed_dict(test_samples))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "            # predicting on all users and items\n",
    "            all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "            print('Step %d: batch/test log loss = %.3f/%.3f, train/test MRR = %.3f/%.3f' % (\n",
    "                    step, loss_value, test_loss_value, \n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, train_df.query(\"rating > 0\"))).mean(),\n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, test_df.query(\"rating > 0\"))).mean()\n",
    "                ))\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        now = dt.datetime.now()\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR + '/movielens-multiclass/train/{:%Y%m%d%H%M%S}'.format(now), sess.graph)\n",
    "        test_summary_writer = tf.summary.FileWriter(LOG_DIR + '/movielens-multiclass/test/{:%Y%m%d%H%M%S}'.format(now), sess.graph)\n",
    "\n",
    "        print('Starting training')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(\n",
    "            tf.variables_initializer(tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\"))\n",
    "        )\n",
    "\n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train_df, test_df, summary_writer)\n",
    "\n",
    "        all_prediction_values = logits.eval(feed_dict={inputs.user_ids: all_user_ids}).ravel()\n",
    "        train_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            train_df.query(\"rating > 0\"))\n",
    "\n",
    "        test_hits = all_predictions_to_hits(\n",
    "            all_user_items, all_prediction_values,\n",
    "            test_df.query(\"rating > 0\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
