{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allaingu/miniconda3/envs/gui/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "original test\n",
      "[1 2 3 4 5]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "train\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 90570 stored elements in COOrdinate format>\n",
      "test\n",
      "[-1  1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 9430 stored elements in COOrdinate format>\n",
      "test_positive_only\n",
      "[1]\n",
      "<943x1682 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5469 stored elements in COOrdinate format>\n",
      "There are 19 distinct item features, with values like ['genre:unknown', 'genre:Action', 'genre:Adventure'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.datasets import fetch_movielens\n",
    "\n",
    "data = fetch_movielens('movielens', indicator_features=False, genre_features=True)\n",
    "\n",
    "print('original train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('original test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "\n",
    "# binarizing traing examples as in the original lightfm paper to use the logistic loss\n",
    "data['train'].data = np.array([-1, 1])[1 * (data['train'].data >= 4)]\n",
    "data['test'].data = np.array([-1, 1])[1 * (data['test'].data >= 4)]\n",
    "\n",
    "# should keep only positive test interactions\n",
    "data['test_positive_only'] = data['test'].copy()\n",
    "data['test_positive_only'].data = 1 *(data['test_positive_only'].data>=1)\n",
    "data['test_positive_only'].eliminate_zeros()\n",
    "\n",
    "train = data['train']\n",
    "test = data['test']\n",
    "test_positives = data['test_positive_only']\n",
    "\n",
    "print('train')\n",
    "print(np.unique(data['train'].data))\n",
    "print(data['train'].__repr__())\n",
    "print('test')\n",
    "print(np.unique(data['test'].data))\n",
    "print(data['test'].__repr__())\n",
    "print('test_positive_only')\n",
    "print(np.unique(data['test_positive_only'].data))\n",
    "print(data['test_positive_only'].__repr__())\n",
    "\n",
    "item_features = data['item_features']\n",
    "tag_labels = data['item_feature_labels']\n",
    "print('There are %s distinct item features, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 3)\n",
      "(1586126, 2)\n",
      "(943,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item\n",
       "0     0     0\n",
       "1     0     1\n",
       "2     0     2\n",
       "3     0     3\n",
       "4     0     4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame.from_dict({\n",
    "        'user': train.row,\n",
    "        'item': train.col,\n",
    "        'rating': train.data,\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame.from_dict({\n",
    "        'user': test.row,\n",
    "        'item': test.col,\n",
    "        'rating': test.data,\n",
    "    })\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "test_user_ids = test_df.user.unique()\n",
    "all_user_ids = train_df.user.unique()\n",
    "all_item_ids = np.unique(data['item_features'].tocoo().row)\n",
    "\n",
    "def to_all_user_items(user_ids, item_ids):\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {'user': np.repeat(user_ids, len(item_ids)),\n",
    "         'item': np.tile(item_ids, len(user_ids))})\n",
    "\n",
    "all_user_items = to_all_user_items(all_user_ids, all_item_ids)\n",
    "print(all_user_items.shape)\n",
    "print(test_user_ids.shape)\n",
    "all_user_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensforflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allaingu/miniconda3/envs/gui/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_predictions_to_hits(all_user_items, all_predicted_values, ground_truth_user_items):\n",
    "    predicted_ratings = all_user_items.assign(predicted_rating=lambda _: all_predicted_values)\n",
    "    predicted_ranks = predicted_ratings.groupby('user')['predicted_rating'].rank(ascending=False, method='max')\n",
    "    predicted_ratings['rank'] = predicted_ranks.values - 1\n",
    "\n",
    "    ground_truth_hits = pd.merge(\n",
    "        left=ground_truth_user_items,\n",
    "        right=predicted_ratings,\n",
    "        on=['user', 'item'], how='left')\n",
    "    return ground_truth_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_predicted_hits(predict_function, ground_truth_df, split_size=1000):\n",
    "    user_ids = ground_truth_df.user.unique()\n",
    "    item_ids = ground_truth_df.item.unique()\n",
    "    user_ids_splits = np.array_split(user_ids, len(user_ids) / split_size)\n",
    "    user_items_splits = (to_all_user_items(user_ids_split, item_ids) for user_ids_split in user_ids_splits)\n",
    "    hits_for_user_splits = [all_predictions_to_hits(\n",
    "            split_user_items, \n",
    "            all_predicted_values=predict_function(split_user_items),\n",
    "            ground_truth_user_items=ground_truth_df[ground_truth_df.user.isin(split_user_items.user.unique())])\n",
    "        for split_user_items in user_items_splits]\n",
    "    return pd.concat(hits_for_user_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(predicted_ranks_df):\n",
    "    return predicted_ranks_df\\\n",
    "        .assign(rec_rank=lambda df:1 / (df['rank'] + 1))\\\n",
    "        .groupby('user')['rec_rank'].max()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(positives_df, batch_size, positive_ratio=.33):\n",
    "    n_positives = int(batch_size * positive_ratio)\n",
    "    n_negatives = batch_size - n_positives\n",
    "    negatives = pd.DataFrame.from_dict({\n",
    "        'user': np.random.choice(all_user_ids, replace=True, size=n_negatives),\n",
    "        'item': np.random.choice(all_item_ids, replace=True, size=n_negatives),\n",
    "        'rating': np.repeat(0, n_negatives)\n",
    "        })\n",
    "    return pd.concat([positives_df.sample(n_positives), negatives], axis=0)\n",
    "\n",
    "# if train has both positives and negatives\n",
    "def sample_batch(positives_and_negatives_df, batch_size):\n",
    "    batch_df = positives_and_negatives_df.sample(batch_size)\n",
    "    return batch_df.assign(rating = lambda df: np.maximum(df.rating, 0))\n",
    "\n",
    "test_samples = sample_batch(train_df, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_USERS, N_ITEMS = train.shape\n",
    "\n",
    "\n",
    "class Placeholders:\n",
    "    \n",
    "    def __init__(self, batch_size=None):\n",
    "        self.user_ids = tf.placeholder(tf.int32, shape=[batch_size], name='user_ids')\n",
    "        self.item_sparse_features = tf.sparse_placeholder(tf.int32, name='item_features')\n",
    "        self.item_ids = tf.placeholder(tf.int32, shape=[batch_size], name='item_ids')\n",
    "        self.ratings = tf.placeholder(tf.float32, shape=[batch_size], name='ratings')\n",
    "\n",
    "    def to_feed_dict(self, user_items_df, with_ratings=False):\n",
    "        features_dict = {\n",
    "            self.user_ids: user_items_df.user.values,\n",
    "            self.item_ids: user_items_df.item.values\n",
    "        }\n",
    "        \n",
    "        if with_ratings:\n",
    "            features_dict[self.ratings] = user_items_df.rating.values\n",
    "\n",
    "        return features_dict\n",
    "\n",
    "\n",
    "class UserItem2BinaryModel:\n",
    "    def __init__(self, dimensionality=30):\n",
    "        self.dimensionality = dimensionality\n",
    "        \n",
    "        with tf.name_scope('B'):\n",
    "            self.user_biases =  tf.Variable(tf.random_normal(shape=[N_USERS, 1], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('user_biases', self.user_biases)\n",
    "\n",
    "        with tf.name_scope('B'):\n",
    "            self.item_biases =  tf.Variable(tf.random_normal(shape=[N_ITEMS, 1], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('item_biases', self.item_biases)\n",
    "\n",
    "        with tf.name_scope('X'):\n",
    "            self.user_factors = tf.Variable(tf.random_normal([N_USERS, self.dimensionality], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('user_factors', self.user_factors)\n",
    "            \n",
    "        with tf.name_scope('Y'):\n",
    "            self.item_factors = tf.Variable(tf.random_normal([N_ITEMS, self.dimensionality], stddev=0.01, mean=0))\n",
    "            tf.summary.histogram('item_factors', self.item_factors)\n",
    "\n",
    "    def user_bias(self, user_ids):\n",
    "        with tf.name_scope('B_user'):\n",
    "            return tf.squeeze(tf.nn.embedding_lookup(params=self.user_biases, ids=user_ids), name='B_user')\n",
    "\n",
    "    def item_bias(self, item_ids):\n",
    "        with tf.name_scope('C_item'):\n",
    "            return tf.squeeze(tf.nn.embedding_lookup(params=self.item_biases, ids=item_ids), name='C_item')\n",
    "\n",
    "    def user_item_product(self, user_ids, item_ids):\n",
    "        with tf.name_scope('X_user'):\n",
    "            batch_user_factors = tf.squeeze(tf.nn.embedding_lookup(self.user_factors, user_ids))\n",
    "        with tf.name_scope('Y_item'):\n",
    "            batch_item_factors = tf.squeeze(tf.nn.embedding_lookup(self.item_factors, item_ids))\n",
    "        with tf.name_scope('dot'):\n",
    "            factors_prediction = tf.reduce_mean(\n",
    "                tf.multiply(batch_user_factors, batch_item_factors), reduction_indices=1)\n",
    "        return factors_prediction\n",
    "\n",
    "    def predictions(self, user_ids, item_ids):\n",
    "        with tf.name_scope('inference'):\n",
    "            return tf.add(\n",
    "                self.user_item_product(user_ids, item_ids), \n",
    "                tf.add(self.user_bias(user_ids), self.item_bias(item_ids), name='biases'),\n",
    "                name='logits')\n",
    "                                 \n",
    "\n",
    "def log_loss(predictions, targets):\n",
    "    \"\"\" targets as one-hot encodings\n",
    "    \"\"\"\n",
    "    with tf.name_scope('log_loss'):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Step 0: batch/test log loss = 0.693/0.692, train/test MRR = 0.087/0.019\n",
      "Step 20: batch/test log loss = 0.680/0.680, train/test MRR = 0.399/0.097\n",
      "Step 40: batch/test log loss = 0.663/0.672, train/test MRR = 0.476/0.133\n",
      "Step 60: batch/test log loss = 0.655/0.659, train/test MRR = 0.494/0.137\n",
      "Step 80: batch/test log loss = 0.641/0.651, train/test MRR = 0.545/0.149\n",
      "Step 100: batch/test log loss = 0.630/0.641, train/test MRR = 0.504/0.124\n",
      "Step 120: batch/test log loss = 0.617/0.642, train/test MRR = 0.518/0.129\n",
      "Step 140: batch/test log loss = 0.610/0.623, train/test MRR = 0.414/0.104\n",
      "Step 160: batch/test log loss = 0.594/0.628, train/test MRR = 0.424/0.099\n",
      "Step 180: batch/test log loss = 0.585/0.619, train/test MRR = 0.506/0.121\n",
      "Step 200: batch/test log loss = 0.582/0.615, train/test MRR = 0.503/0.123\n",
      "Step 220: batch/test log loss = 0.579/0.604, train/test MRR = 0.525/0.129\n",
      "Step 240: batch/test log loss = 0.556/0.611, train/test MRR = 0.536/0.136\n",
      "Step 260: batch/test log loss = 0.563/0.603, train/test MRR = 0.540/0.138\n",
      "Step 280: batch/test log loss = 0.568/0.597, train/test MRR = 0.542/0.137\n",
      "Step 300: batch/test log loss = 0.558/0.591, train/test MRR = 0.538/0.136\n",
      "Step 320: batch/test log loss = 0.547/0.611, train/test MRR = 0.535/0.132\n",
      "Step 340: batch/test log loss = 0.544/0.591, train/test MRR = 0.528/0.130\n",
      "Step 360: batch/test log loss = 0.569/0.583, train/test MRR = 0.528/0.131\n",
      "Step 380: batch/test log loss = 0.545/0.559, train/test MRR = 0.519/0.128\n",
      "Step 400: batch/test log loss = 0.542/0.596, train/test MRR = 0.486/0.122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "N_ITER = 401\n",
    "BATCH_SIZE = 1024\n",
    "N_STEP_SUMMARY = 20\n",
    "LOG_DIR = '/tmp/tfrecs_logs'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = UserItem2BinaryModel(dimensionality=10)\n",
    "    inputs = Placeholders()\n",
    "    \n",
    "    logits = model.predictions(inputs.user_ids, inputs.item_ids)\n",
    "    \n",
    "    loss = log_loss(logits, inputs.ratings)\n",
    "    tf.summary.scalar('train_loss', loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    test_summary = tf.summary.scalar('test_loss', loss)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "                \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        batch_samples = sample_batch(train_df, BATCH_SIZE)\n",
    "\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict=inputs.to_feed_dict(batch_samples, with_ratings=True))\n",
    "        \n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % N_STEP_SUMMARY == 0:\n",
    "\n",
    "            test_samples = sample_batch(test_df, BATCH_SIZE)\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict=inputs.to_feed_dict(test_samples, with_ratings=True))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "            # predicting on all users and items\n",
    "            all_prediction_values = logits.eval(feed_dict=inputs.to_feed_dict(all_user_items))\n",
    "            \n",
    "            print('Step %d: batch/test log loss = %.3f/%.3f, train/test MRR = %.3f/%.3f' % (\n",
    "                    step, loss_value, test_loss_value, \n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, train_df.query(\"rating > 0\"))).mean(),\n",
    "                    mean_reciprocal_rank(all_predictions_to_hits(\n",
    "                        all_user_items, all_prediction_values, test_df.query(\"rating > 0\"))).mean()\n",
    "                ))\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(LOG_DIR + 'movielens-binary/{:%Y%m%d%H%M%S}'.format(dt.datetime.now()), sess.graph)\n",
    "\n",
    "        print('Starting training')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train_df, test_df, summary_writer)\n",
    "\n",
    "        train_hits = all_predicted_hits(\n",
    "            lambda user_items: logits.eval(inputs.to_feed_dict(user_items)),\n",
    "            train_df, split_size=100)\n",
    "\n",
    "        test_hits = all_predicted_hits(\n",
    "            lambda user_items: logits.eval(inputs.to_feed_dict(user_items)),\n",
    "            test_df.query(\"rating > 0\"), split_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 5)\n",
      "(5469, 5)\n",
      "0.5049011863113853\n",
      "0.1216828731854139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.648105</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076002</td>\n",
       "      <td>488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.677963</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511360</td>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.242556</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating  predicted_rating   rank\n",
       "0     0    19       1          0.648105  303.0\n",
       "1     0    32       1          0.076002  488.0\n",
       "2     0    60       1          0.677963  294.0\n",
       "3     0   159       1          0.511360  348.0\n",
       "4     0   170       1          1.242556  139.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_hits.shape)\n",
    "print(test_hits.shape)\n",
    "        \n",
    "print(mean_reciprocal_rank(train_hits).mean())\n",
    "print(mean_reciprocal_rank(test_hits).mean())\n",
    "test_hits.head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
